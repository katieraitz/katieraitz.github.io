Implicit networks, and specifically [Deep Equilibrium Models (DEQs)](https://arxiv.org/abs/1909.01377), have been an emerging architecture in DL over the past 5 or so years in response to memory efficiency and depth concerns with large and complex datasets. They define their output as a solution to a fixed-point equation or, implicitly by the network's parameters and inputs, rather than through the explicit sequential computations done in traditional feedforward networks, which compute outputs layer-by-layer. This is beneficial as training comes at a constant memory cost, even for potentially infinite-depth networks. However, the implicit nature of the fixed point problem solving leads to the requirement of a Jacobian computation during backpropagation. The Jacobian matrix holds data on the relationships between the small changes in the inputs and outputs of the system defined by the equilibrium condition of a fixed point problem.

Since my research project will be involving a special cost-saving approach to this implicit network bottleneck, I spent my 2nd week following the rabbit hole backwards to why it would be motivated in the first place. I read through the journal articles that have used DEQs to understand what kind of problems they excel at: problems where depth and continuous transformations are crucial, such as image processing, differential equation solving for blackbox dynamic systems, and generative modeling of complex distributions. I also learned more about why optimization comes into play here so poignantly as implicit networks define outputs implicitly, having to satisfy some constraint(s) in order to directly compute gradients at the solution point of the equation. This is an optimization sub-problem differing from traditional NNs where the function that transforms the output is explicitly defined.

To better prepare for my research question, I need to understand the different methods of gradient descent (GD) during the training step of backpropagation in DL. The backpropagation methods range from stochastic GD (using one training example at a time), minibatch GD (using several training examples at a time, but smaller than the entire dataset), and batch GD (processing the entire dataset to update the weights). [Minibatch GD](https://arxiv.org/abs/1804.07612) has become the norm in DL, and I searched the aforementioned papers for mention of the different optimization settings used in DEQs, and most seemed to focus on this method of stochastic minibatches for training. However, it was difficult for me to discern which GD methods were used in some of the resources I accessed.

While it's true that minibatches can provide computational relief, could we feasibly experiment with batch GD if we used a [cost-saving approach](https://arxiv.org/abs/2103.12803) in the training of our DEQs? Additionally, I am still unsure of how performance will be impacted relative to DEQs. My literature review revealed that fine-grained (stochastic optimization) updates achieve convergence for these networks, and given the inner loop of iterative computations to find the equilibrium state, updating the parameters based on the entire dataset (batch GD) *might* lead to updates that overshoot the equilibrium state or create oscillations around it... but I aim to research more about how we could leverage batch GD to an advantage and actually improve stability of gradient updates.

I feel I better understand the differences between traditional feedforward deep learning networks and implicit networks/DEQs. I understand better about how the different GD methods impact number of passes required before completing an epoch. Ultimately, my research question is: given the success of JFB in DL models relying on stochasticity, will this success still apply with standard deterministic optimization (no minibatch, no noise added)? Which optimization setting is better for JFB?
