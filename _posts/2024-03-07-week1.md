---
layout: post
title: Week 1
---

I'm interested in ML and how to be a better CS practitioner with choosing, training, and evaluating these algorithms and their results. Specifically, I want to better understand deep learning (DL), the sophisticated and widely successful architecture useful for large datasets and complex problems such as image classification. While I have used Keras, NumPy, Scikit-Learn, and Pandas libraries for data processing and to build and train NNs, I have been left feeling like the training was a black box to guess and check against, and to tinker with parameters until I reach a "good enough" solution. So I am taking a more rigid, mathematical approach to reaching results.

Under the tutelage of my advisor, I have spent many months learning the prerequisite foundational maths involved in DL optimization: calculus, linear algebra, and numerical methods of approximation. Now, my focus with this research will be on optimizing the gradient calculation needed in NN training. Specifically, I'm interested in exploring the batch normalization method, and how it might be impacting the training step. I will be looking into a specific type of backpropagation called Jacobian-free backpropagation, an efficient technique saving on both time and memory storage requirements. A major focus in CS is balancing the classic trade-off of accuracy with efficiency, and the motivation behind my research is the understanding that we can use numerical optimization to advance these requirements in the field of DL.

I have been attending the Mines Optimization and Deep Learning (MODL) weekly seminars with a research group led by my advisor and another AMS faculty member, Dr. Daniel McKenzie. In these seminars, with both faculty and graduate students giving presentations, I have learned more about methods for hyperparameter tuning, generative adversarial networks, spatial processes for NN, and fixed point diffusion models. I actually gave an informal presentation/led a discussion on implicit graph NNs after reading a [paper on the topic](https://arxiv.org/abs/2009.06211). Additionally, I have been attending as a guest the graduate Numerical Optimization Special Topics:MATH course being held this Spring, which has been going into more depth on the construction of different families of optimization techniques. I have been met with constant support from all of the members involved in the AMS department, and I've been receiving a lot of help in grasping some of these concepts. I'm excited to continue to gain better understanding in this atmosphere.
