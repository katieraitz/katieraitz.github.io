This week I spent time doing PyTorch tutorials to prepare me to work with my project. Because I am working with a novel DEQ network using JFB, I will need to be creative in my coding to set the parameters correctly. I haven't coded in PyTorch before, and my math programming skills are not strong, so I started with tutorials that worked up from vanilla NN up into DL networks.

I met with my advisor and discussed how JFB works, with approximating the gradient via substituting the Identity matrix for the Jacobian. The reason this works is because of the nature of implicit differentiation. Additionally, I met with Dr. Daniel McKenzie to talk about the JFB framework. I was enlightened more to the difference between theory and application. Having this experience to do research is exposing me more to how theoretical guarantees are applied in a different manner. For instance, this comes into play when we consider mini-batching in real world applications and how full batching is often infeasible; and when we don't directly compute the Jacobian even in Jacobian-based backpropagation. It often comes down to hardware constraints, which motivates why JFB is an attractive approach in practice. I additionally met with some PhD students working in DL and FP theory, and I got some clarifications on the corrected understanding of what I've been researching thus far.
