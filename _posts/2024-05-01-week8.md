I completed my project code and research paper detailing the background and my process. It was found that deterministic optimization holds up in performance and excels in timing resources when paired with JFB. Training time was significantly faster than the smaller batch sizes, which had comparable loss minimzations and accuracy measurements. Since these mini-batches were less than 1% of the dataset size, the noise introduced caused oscillations in convergence and significantly more iterations needed to reach desired values, making the full pass of the training process much more lengthy. Full-batch processing easily achieved greater performance than the larger mini-batch sizes by achieving a significantly lower loss than these same shorter-training-time batch sizes (though comparable loss to the two smallest batch sizes that took much longer to train). Perhaps less importantly, the model took more epochs to reach this lowest level of loss: this should be kept in mind when tuning hyperparameters. The gradient descent of our full-batch was a smooth cost function, indicating reliability in computation and less sharp minima distracting away from a global minimum.

Taken together, the findings suggest JFB frameworks being trained deterministically need to be trained with a higher number of epochs, and with the correct hyperparameter tunings, can provide a good balance of stability, accuracy, and generalization. I found this research to be very interesting, and enjoyed getting the data science experience of using visualizations to communicate a message. Additionally, I gained a feel for the culture of academia and research, which to me is characterized by the open-endedness of work. There aren't structured problems to solve like when being a typical student. I am grateful to my mentor and other faculty who helped me learn so much during this project, as well as the graduate students I worked closely with. The DREAM experience was a great one for me!
