While I have been collecting understanding of JFB in my research, beginning the coding part of my project in PyTorch has given new depth of familiarity. Translating the mathematical theory behind DEQs utilizing JFB into a coded-up model meant understanding that implicit networks always have input injection... This means that instead of propagating outcomes and feeding only the previous layer’s outcome into the next layer’s input (like feedforward NN (FFNN)), we additionally feed the original input in to each layer such that the training parameters are transferable between layers. This makes the latent training space weight-tied, and so this architecture applies its transformation differently as it is applied iteratively in a loop.

In coding this, I tinkered with what was happening under the hood of PyTorch which saves tensor objects to track computational graphs of the network’s training data. Understanding the weight-tied, input injected aspects of the NN was critical in deciding how to parameterize my network. Coding the latent space proved to be a little different in a network utilizing fixed-point iteration than a FFNN, too, as the layers within the latent space are essentially dealt with by PyTorch after I have coded up the loop, conditions, and transformation action. I began with training my implicit neural network to approximate a 1D function and experimented with tuning all the hyperparameters before moving to more complex functions. Working with the code helped me understand better the memory requirement issues of Jacobian-based backprop in implicit networks versus JFB.
