DEQs utilize **fixed point iteration** to converge upon a fixed point while computing a solution during forward propagation, and we call this period **inference**. Inference stands in contrast to traditional deep NNs computing a solution during backpropagation where the parameters are adjusted to get better performance. Solving the fixed point problem avoids the need to store intermediate states of the parameters, and consequently requires only O(1) memory consumption during training.

Fixed point iterative methods involve iteratively applying a function until the output does not change significantly, indicating that the equilibrium has been reached. In the context of DEQs, this function actually refers to the architecture of the network itself, or the transformation it represents, defined by the network's layers, weights, and activation functions. The goal is to design an architecture such that, for any given input, the network can iteratively apply its transformation and converge to a stable state, where the input to the transformation equals its output - otherwise known as finding a fixed point. Guaranteeing a fixed point means ensuring that the network's architecture is designed in such a way that it _always_ reaches a stable equilibrium for _any_ given input.

It's clear that fixed point theory is the fundamental workhorse behind DEQs, which as a model, was indeed motivated by the observation that the hidden layers of many deep sequence models tend to converge towards some fixed point when the same transformation is applied repeatedly. There are some ideas from functional analysis that are used in ensuring these conditions, and to formulaically guarantee a fixed point, **Banach's Fixed Point Theorem** must be understood and applied.

Fixed Point Theorem (developed in 1922) relies on the **Banach Contraction Principle**, which guarantees the existence and uniqueness of fixed points under certain conditions, and it provides a method for finding these fixed points. A **metric space** is a set equipped with a metric, which is a function that defines the distance between every pair of points in the set. A function on a nonempty, complete metric space is what's called a **contraction mapping** if there exists a constant factor between 0 (inclusive) and 1 (exclusive) that keeps distance between inputs greater than or equal to distance between outputs. In other words, through the iterative method, the outputs stay the same distance or grow closer and closer by a certain constant proportion. Consequently, after a certain number of iterations, the output stops changing: and this is our fixed point.

I spent this week trying to understand fixed point theory better, so that I can move towards designing an implicit network for my project. In the past, as I have explored this concept, I have had meetings with my advisor, another AMS faculty member: Dr. Daniel McKenzie, an AMS professor of Analysis courses: Dr. John Griesmer, and a PhD AMS student to understand contractiveness. It's an ubiquitous concept that ultimately relies on comparison to geometric series, but it's been difficult for me to wrap my head around its theoretical importance. After reading more papers and these discussions, I don't feel as mystified by it. However, I intend to understand it better as I continue to work with it and apply it.
