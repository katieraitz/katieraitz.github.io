I attended a lecture given by my mentor on Stochastic Optimization, in which he detailed why stochastic gradient descent (SGD) is so ubiquitously used in DL. It was discussed how SGD often can’t be applied unless the problem at hand is an empirical risk problem (instead of evaluating an expected risk). Expected risk involves the true data distribution, which is often not feasible in practice if the data distribution is unknown. Conversely, empirical risk minimization is considered when the risk is calculated from the data available, or just a sample, in order to minimize the average loss over the entire data distribution. This average results in a discrete approximation of the expected risk. We can see here that the layers of DL models are inherently structured in a conducive nature to empirical risk minimization. However, it is still important for DL models to handle more complex problems where precise predictions are essential, such as those encountered in physics modeling, where the objective often extends beyond empirical risk minimization to more accurately model expected outcomes. These kinds of problems generate highly structured and noise-sensitive data which involve differential equations describing continuous processes. Stochastic methods optimize in a way that can be noisy and may not converge smoothly or to the local minima when precise adjustments to model parameters are needed: they inherently introduce randomness and approximations through minibatch sampling. Full-batch GD, although computationally heavier, can provide the level of precision or stability required for solving these equations accurately as it considers all data points for each update, thus providing a stable and precise error gradient.

My main takeaway from that lecture is that there is a tradeoff between precise optimization and robust generalization, and often in DL, good generalization is the more important net goal. The insights from this lecture motivate my project, as for many problems, SGD is helpful in introducing noise to the learning process, but using larger batches for a more controlled gradient calculation can effectively address the aforementioned demands. Space complexity is also impacted when SGD treats the sample as a distribution, updating the model weights more frequently and extracting more information. In this way, taking the loss gradient is not the expensive part: storing all the gradients is. In this project, I am exploring the use of a deterministic method, or using the sample average, on a space-saving technique, and with these larger batches giving a more controlled gradient, we can address the limitations of SGD to applications requiring high precision. I am contributing to the understanding of the bias/variance tradeoff involved with this DEQ technique, as understanding which settings JFB is effective in can lead to better capability in applying it.

I worked with a PhD student on understanding more about PyTorch and correcting my code. He helped me visualize the differences between the mathematical theory and what’s going on in the code through drawings, and he also encouraged me to use plotting for my model evaluation. I also did some visualization of the model’s predictions across epochs to see how the model's predictions were evolving during training. In gaining more of an understanding of my network, I also adjusted it to approximate a more complex 3D function, and continued tuning the hyperparameters.
