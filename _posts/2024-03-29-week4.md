Traditional ML models employ feature scaling as a preprocessing step applied to the input data before it is fed into the model, to normalize or standardize the features of the data so that they have specific statistical properties. The main point of this is to ensure that no particular feature dominates the learning process due to its scale. In DL, **Batch Normalization (BatchNorm)** is typically a layer incorporated during training that similarly introduces a form of dependency between inputs, that wasn't present in the raw data or in the activations prior to its application. BatchNorm normalizes the input layer by adjusting and scaling the activations, ensuring that the maximum and minimum values are scaled to the same range. This accelerates the training by stabilizing their distributions and hence gradient calculation for the loss function, resulting in smooth and more predictable updates. It also improves performance by reducing **internal covariance shift**, which is a bigger problem in deep networks, where the distribution of each layer's inputs changes during training as the parameters of the previous layers change.
Implicit models, especially those operating in a **latent space**, often deal with complex optimization landscapes. The latent space is the representation of compressed data positioning similar data points closer together in space. The latent space might contain numerous local minima, saddle points, or flat regions, making it challenging for the optimization algorithm to find a global minimum. If the model's training continually oscillates or gets stuck in a less optimal point, this is considered a failure of convergence. This potential numerical instability arises from the complex, direct computation and inversion of large Jacobian matrices, which are used in the propagation of gradients through the network transformations that take place in these latent spaces.
JFB-based implicit networks have notably circumvented the limitation observed in Jacobian-based implicit networks of successfully implementing BatchNorm in the latent space. Because BatchNorm mitigates issues related to vanishing or exploding gradients, it enables the training of much deeper networks than would otherwise be feasible. Other Jacobian-based methods have been unable to apply BatchNorm, as they have failed to converge and provide a descent direction (even with tighter tolerance and more iterations). This is important because it means JFB works with a wider range of architectures, as BatchNorm is a critical component in many successful DL models.
BatchNorm introduces dependencies between the inputs of a layer: this means the output from a given input depends not only on the input itself, but also on the other inputs in the batch... so the fixed point for one input in the batch can impact and be impacted by others, making finding a consistent fixed point more challenging. This variability and hence instability in the equilibrium computation increases for smaller batch sizes where the statistics become noisy. In this way, BatchNorm effectiveness is closely tied to the batch size: when the batch size is small, it can introduce unwanted noise into the training process. However, with the full batch being used in training, the dependencies are constant for the entire training dataset rather than varying with each mini-batch. This can make the model's training behavior more predictable but might also reduce its ability to generalize from the training data to unseen data due to the reduction in noise and regularization effect. Additionally, deterministic training requires high precision and stability, both of which are conducive to the finding of a fixed point. In other words, [training with the full-batch provides direct access to each and every fixed point](https://proceedings.neurips.cc/paper_files/paper/2021/file/4ffbd5c8221d7c147f8363ccdc9a2a37-Paper.pdf), which can allow not only achieved, but accelerated convergence.
It's important here to understand the derivative as a ratio quantifying how sensitive an output will be to small changes in the input. Forcing more stable distributions of inputs across layers controls sensitivity to initialization and subsequent scale of inputs: magnitudes of inputs and intermediate activations do not become too large or too small. With this, the training process is accelerated in the fixed-point finding process. When designing a DEQ, understanding and controlling the sensitivity of the model's output to its inputs (and intermediate states) is crucial because of the contractive mapping condition, which, in simplified terms, requires the derivatives (or norms of the Jacobian matrix) to be bounded above by 1.
Introducing BatchNorm is crucial for tasks that benefit from deep representations as it influences the loss gradient and model updates during backpropagation, but it also requires careful balancing to ensure it doesn't introduce too much noise into the gradient estimation, which is especially important in models where the equilibrium state determines the output. At this point, I have learned about the different background methods to this project, and I will now be addressing the project and coding to train an implicit network with JFB. I plan to start meeting more regularly with my advisor and other professors and graduate students for assistance in this experiment. Regarding my research this week, I understand that the calculation of batch statistics (mean and variance) in BatchNorm requires a clear delineation of layers and a forward pass through these layers. In DEQs, where the concept of layers is abstracted away in favor of finding a fixed point, it's unclear how or where to correctly apply BatchNorm, since the equilibrium state in DEQs is determined by iterating until convergence rather than a straightforward pass through defined layers. I plan to understand more about the latent space and how this enables BatchNorm in a DEQ model. My other questions now are: how responsible is BatchNorm for the speed and accuracy of JFB-based implicit networks? How did tuning weights in JFB networks encourage contractive behavior automatically? How will the speed of the training process be impacted by full batch training without the regularizing effects of mini-batch noise?
